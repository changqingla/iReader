"""Token counting utilities for content management."""
from pathlib import Path
from typing import Tuple, Optional

import transformers
from src.utils.logger import get_logger

logger = get_logger(__name__)

_QWEN_TOKENIZER: Optional[transformers.PreTrainedTokenizer] = None
_TOKENIZER_LOADED = False


def _get_qwen_tokenizer() -> transformers.PreTrainedTokenizer:
    """
    èŽ·å–æˆ–åˆ›å»ºQwen tokenizerï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    ä½¿ç”¨å…¨å±€ç¼“å­˜é¿å…æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°åŠ è½½tokenizerï¼ˆæå‡æ€§èƒ½çº¦1000å€ï¼‰
    
    Returns:
        transformers.PreTrainedTokenizer: ç¼“å­˜çš„tokenizerå®žä¾‹
    """
    global _QWEN_TOKENIZER, _TOKENIZER_LOADED
    
    if _QWEN_TOKENIZER is None:
        if not _TOKENIZER_LOADED:
            logger.info("é¦–æ¬¡åŠ è½½ Qwen tokenizerï¼Œè¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ...")
            _TOKENIZER_LOADED = True
        
        # tokenizer åœ¨ context/tokenizer ç›®å½•
        tokenizer_dir = Path(__file__).parent / "tokenizer"
        
        if not tokenizer_dir.exists():
            error_msg = (
                f"æœªæ‰¾åˆ° Qwen tokenizer ç›®å½•: {tokenizer_dir}\n"
                f"è¯·ç¡®ä¿ tokenizer æ–‡ä»¶å­˜åœ¨äºŽ {tokenizer_dir}"
            )
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        try:
            _QWEN_TOKENIZER = transformers.AutoTokenizer.from_pretrained(
                str(tokenizer_dir),
                trust_remote_code=True,
                local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            logger.info(f"âœ… Qwen tokenizer åŠ è½½å®Œæˆï¼ˆè·¯å¾„: {tokenizer_dir}ï¼‰ï¼Œå·²ç¼“å­˜")
        except Exception as e:
            logger.error(f"åŠ è½½ Qwen tokenizer å¤±è´¥: {e}")
            raise
    
    return _QWEN_TOKENIZER


def calculate_tokens(text: str, model: str = "Qwen/Qwen3-30B-A3B-Instruct-2507") -> int:
    """
    ä½¿ç”¨ transformers è¿›è¡Œ token è®¡ç®—ï¼ˆQwen æ¨¡åž‹ï¼‰
    
    ä¼˜åŒ–ï¼šä½¿ç”¨å…¨å±€ç¼“å­˜çš„ tokenizerï¼Œé¿å…æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°åŠ è½½ï¼ˆæå‡æ€§èƒ½çº¦1000å€ï¼‰
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        model: æ¨¡åž‹åç§°ï¼ˆå½“å‰ä½¿ç”¨æœ¬åœ°tokenizerï¼Œå¿½ç•¥modelå‚æ•°ï¼‰
        
    Returns:
        tokenæ•°é‡
    """
    # ç©ºæ–‡æœ¬æ£€æŸ¥
    if text is None or not text:
        return 0
    
    try:
        tokenizer = _get_qwen_tokenizer()
        
        # ç¼–ç æ–‡æœ¬
        result = tokenizer.encode(text)
        return len(result)
        
    except FileNotFoundError:
        # tokenizer æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç²—ç•¥ä¼°ç®—
        logger.warning("Tokenizer æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç²—ç•¥ä¼°ç®—")
        estimated = len(text)
        return estimated
        
    except Exception as e:
        logger.error(f"Token è®¡ç®—å¤±è´¥: {e}")
        # Last resort: rough estimation (1 token â‰ˆ 4 chars)
        estimated = len(text)
        logger.warning(f"ä½¿ç”¨ç²—ç•¥ä¼°ç®—: {estimated} tokens")
        return estimated


def should_use_direct_content(
    content: str,
    available_tokens: int,
    threshold: float = 0.7,
    model: str = "gpt-4"
) -> Tuple[bool, int]:
    """
    Determine if content should be used directly or if recall is needed.
    
    Args:
        content: The full document content
        available_tokens: Maximum available tokens for the context
        threshold: Threshold ratio (default: 0.7 means use up to 70% of available tokens)
        model: Model name for token calculation
        
    Returns:
        Tuple of (should_use_direct, token_count)
        - should_use_direct: True if content can be used directly
        - token_count: Number of tokens in the content
    """
    # Calculate tokens in the content
    token_count = calculate_tokens(content, model)
    
    # ðŸ”‘ Safety check: handle zero or negative available_tokens
    if available_tokens <= 0:
        logger.warning(f"âš ï¸ available_tokens is {available_tokens}, cannot use direct content")
        logger.warning("Content will be processed via recall")
        return False, token_count
    
    # Calculate maximum allowed tokens
    max_allowed_tokens = int(available_tokens * threshold)
    
    # Determine if content is small enough to use directly
    should_use = token_count <= max_allowed_tokens
    
    # Log the decision
    percentage = (token_count / available_tokens) * 100
    logger.info(f"Token analysis: {token_count:,} tokens / {available_tokens:,} available "
               f"({percentage:.1f}%, threshold: {threshold*100:.0f}%)")
    
    if should_use:
        logger.info("âœ… Content is small enough for direct use")
    else:
        logger.info("âš ï¸ Content exceeds threshold, recall recommended")
    
    return should_use, token_count


if __name__ == "__main__":
    # with open("/mnt/general/ht/deep_doc/æµ‹è¯•.md", "r", encoding="utf-8") as f:
    #     content = f.read()
    content = "æ€»ç»“ä¸€ä¸‹è¿™ç¯‡è®ºæ–‡"
    available_tokens = 1000
    threshold = 0.7
    model = "Qwen/Qwen3-30B-A3B-Instruct-2507"
    should_use, token_count = should_use_direct_content(content, available_tokens, threshold, model)
    print(f"should_use: {should_use}, token_count: {token_count}")