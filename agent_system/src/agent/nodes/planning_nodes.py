"""è§„åˆ’ç›¸å…³èŠ‚ç‚¹"""
from typing import Dict, Any, List, AsyncGenerator, Optional

from langchain_core.messages import HumanMessage

from .base import BaseAgentNode
from ..state import AgentState, IntentType, Plan, SubQuestion
from ...prompts import (
    INTENT_RECOGNITION_PROMPT,
    SUB_QUESTION_GENERATION_PROMPT,
)
from ...utils.logger import get_logger
from ...utils.json_parser import parse_json_response
from ..constants import MAX_INTENT_RECOGNITION_RETRIES

logger = get_logger(__name__)


class PlanningNodes(BaseAgentNode):
    """æ„å›¾è¯†åˆ«ã€ç­–ç•¥é€‰æ‹©å’Œè®¡åˆ’ç”ŸæˆèŠ‚ç‚¹"""
    
    # æ„å›¾ç±»å‹çš„ä¸­æ–‡æ˜¾ç¤ºåç§°æ˜ å°„
    INTENT_DISPLAY_NAMES = {
        "LITERATURE_SUMMARY": "æ–‡çŒ®æ€»ç»“",
        "REVIEW_GENERATION": "ç»¼è¿°ç”Ÿæˆ",
        "LITERATURE_QA": "æ–‡çŒ®é—®ç­”",
        "DOCUMENT_COMPARISON": "æ–‡ç« å¯¹æ¯”",
        "GENERAL_TASK": "é€šç”¨ä»»åŠ¡"
    }
    
    # Pipeline è·¯çº¿æ”¯æŒçš„æ„å›¾ç±»å‹
    PIPELINE_INTENTS = {
        IntentType.LITERATURE_SUMMARY,
        IntentType.REVIEW_GENERATION,
        IntentType.LITERATURE_QA,
        IntentType.DOCUMENT_COMPARISON
    }
    
    def _get_intent_display_name(self, intent: str) -> str:
        """å°†æ„å›¾ç±»å‹è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„æ˜¾ç¤ºåç§°"""
        return self.INTENT_DISPLAY_NAMES.get(intent, intent)
    
    async def intent_recognition_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æ„å›¾è¯†åˆ«èŠ‚ç‚¹"""
        try:
            mode_type = state.get("mode_type")
            if mode_type:
                try:
                    detected_intent = IntentType(mode_type)
                    route = "pipeline" if detected_intent in self.PIPELINE_INTENTS else "react"
                    # æ„å›¾è¯†åˆ«ç»“æœåªè®°å½•æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°å‰ç«¯
                    logger.info(f"ä½¿ç”¨æŒ‡å®šçš„ä»»åŠ¡ç±»å‹: {mode_type} â†’ {route}")
                    
                    document_ids = state.get("document_ids", [])
                    doc_count = len(document_ids) if document_ids else 0
                    yield {"type": "node_complete", "data": {"detected_intent": detected_intent, "doc_count": doc_count, "route": route}}
                    return
                except ValueError:
                    logger.warning("æä¾›çš„ä»»åŠ¡ç±»å‹æ— æ•ˆï¼Œè°ƒç”¨LLMè¯†åˆ«")
            
            context_str = await self._get_conversation_context_async(state, stage="intent_recognition")
            
            # è·å–æ–‡æ¡£ä¿¡æ¯
            document_ids = state.get("document_ids", [])
            doc_count = len(document_ids) if document_ids else 0
            has_documents = "true" if doc_count > 0 else "false"
            
            # è·å–åŒ—äº¬æ—¶é—´
            from datetime import datetime, timezone, timedelta
            beijing_tz = timezone(timedelta(hours=8))
            current_time = datetime.now(beijing_tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
            
            prompt = INTENT_RECOGNITION_PROMPT.format(
                current_time=current_time,
                user_query=state["user_query"],
                conversation_history=context_str if context_str else "æ— ",
                has_documents=has_documents,
                document_count=doc_count
            )
            
            # æ”¶é›†å®Œæ•´å“åº”ï¼ˆä¸è¾“å‡ºä¸­é—´çŠ¶æ€ï¼‰
            
            full_response = ""
            async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_response += chunk_content
            
            detected_intent = None
            reasoning = ""
            confidence = 0
            
            for attempt in range(MAX_INTENT_RECOGNITION_RETRIES):
                parsed = parse_json_response(full_response, expected_fields=["intent", "reasoning"])
                
                if parsed and "intent" in parsed:
                    try:
                        detected_intent = IntentType(parsed["intent"])
                        reasoning = parsed.get("reasoning", "")
                        confidence = parsed.get("confidence", 0)
                        break
                    except ValueError:
                        pass
                
                intent_str = full_response.strip()
                try:
                    detected_intent = IntentType(intent_str)
                    break
                except ValueError:
                    pass
                
                if attempt < MAX_INTENT_RECOGNITION_RETRIES - 1:
                    full_response = ""
                    async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                        chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                        full_response += chunk_content
            
            if detected_intent is None:
                # é»˜è®¤ä½¿ç”¨ GENERAL_TASKï¼ˆç”± ReAct å¤„ç†ï¼‰
                detected_intent = IntentType.GENERAL_TASK
            
            # æ„å›¾è¯†åˆ«ç»“æœåªè®°å½•æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°å‰ç«¯
            if reasoning:
                logger.info(f"æ„å›¾è¯†åˆ«æ¨ç†: {reasoning}")
            
            document_ids = state.get("document_ids", [])
            doc_count = len(document_ids) if document_ids else 0
            
            # ç¡®å®šè·¯ç”±ï¼šPipeline æˆ– ReAct
            route = "pipeline" if detected_intent in self.PIPELINE_INTENTS else "react"
            
            display_name = self._get_intent_display_name(detected_intent.value)
            route_display = "ä¸“ç”¨æµæ°´çº¿" if route == "pipeline" else "ReAct Agent"
            logger.info(f"è¯†åˆ«æ„å›¾: {display_name} â†’ {route_display}")
            
            yield {"type": "node_complete", "data": {"detected_intent": detected_intent, "doc_count": doc_count, "route": route}}
        except Exception as e:
            logger.error(f"Error in intent_recognition_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "intent_recognition", "error": str(e)}
    
    async def strategy_selection_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ç­–ç•¥é€‰æ‹©èŠ‚ç‚¹"""
        try:
            if state.get("use_direct_content", False):
                result = {"strategy": "full_content", "use_direct_content": True}
            else:
                document_count = state.get("doc_count", 0)
                detected_intent = state.get("detected_intent")
                
                if document_count > 1:
                    if detected_intent in [IntentType.LITERATURE_SUMMARY, IntentType.REVIEW_GENERATION, IntentType.DOCUMENT_COMPARISON]:
                        result = {"strategy": "multi_doc_summary", "use_direct_content": False}
                    else:
                        result = {"strategy": "chunk_recall", "use_direct_content": False}
                else:
                    result = {"strategy": "chunk_recall", "use_direct_content": False}
            
            # æ‰“å°ç­–ç•¥é€‰æ‹©æ—¥å¿—
            logger.info("=" * 60)
            logger.info("ğŸ¯ ç­–ç•¥é€‰æ‹©ç»“æœ:")
            logger.info(f"   - æ–‡æ¡£æ•°é‡: {state.get('doc_count', 0)}")
            logger.info(f"   - è¯†åˆ«æ„å›¾: {state.get('detected_intent')}")
            logger.info(f"   - é€‰æ‹©ç­–ç•¥: {result.get('strategy')}")
            logger.info("=" * 60)
            
            yield {"type": "node_complete", "data": result}
        except Exception as e:
            logger.error(f"Error in strategy_selection_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "strategy_selection", "error": str(e)}
    
    async def sub_question_generation_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """å­é—®é¢˜ç”ŸæˆèŠ‚ç‚¹"""
        try:
            strategy = state.get("strategy", "chunk_recall")
            if strategy != "chunk_recall":
                yield {"type": "node_complete", "data": {}}
                return
            
            user_query = state["user_query"]
            doc_count = state.get("doc_count", 0)
            document_ids = state.get("document_ids", [])
            document_names = state.get("document_names", {}) or {}
            
            # æ„å»ºæ–‡æ¡£åˆ—è¡¨ï¼šä½¿ç”¨æ–‡æ¡£åç§°è€Œä¸æ˜¯ IDï¼ˆå¯¹ LLM æ›´æœ‰æ„ä¹‰ï¼‰
            if document_ids:
                doc_entries = []
                for doc_id in document_ids:
                    doc_name = document_names.get(doc_id, f"æ–‡æ¡£_{doc_id[:8]}")
                    doc_entries.append({"id": doc_id, "name": doc_name})
                import json
                doc_list_display = json.dumps(doc_entries, ensure_ascii=False)
            else:
                doc_list_display = "[]"
            
            # ä½¿ç”¨ % æ ¼å¼åŒ–é¿å…ä¸ JSON ä¸­çš„èŠ±æ‹¬å·å†²çª
            prompt = SUB_QUESTION_GENERATION_PROMPT.replace(
                "{user_query}", user_query
            ).replace(
                "{doc_type}", "å­¦æœ¯æ–‡çŒ®"
            ).replace(
                "{need_context}", "å¦"
            ).replace(
                "{mode}", "Fast"
            ).replace(
                "{document_count}", str(doc_count)
            ).replace(
                "{document_list}", doc_list_display
            )
            
            # æ”¶é›†å®Œæ•´å“åº”ï¼ˆä¸è¾“å‡ºä¸­é—´çŠ¶æ€ï¼‰
            
            full_response = ""
            async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_response += chunk_content
            
            sub_questions = parse_json_response(full_response, expected_fields=None)
            
            if not isinstance(sub_questions, list) or len(sub_questions) == 0:
                result = {}
            else:
                result = {"sub_questions": sub_questions}
            
            yield {"type": "node_complete", "data": result}
        except Exception as e:
            logger.error(f"Error in sub_question_generation_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "sub_question_generation", "error": str(e)}
    
    async def plan_generation_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        è®¡åˆ’ç”ŸæˆèŠ‚ç‚¹
        
        å°†å­é—®é¢˜è½¬æ¢ä¸ºæ‰§è¡Œè®¡åˆ’ï¼Œå¦‚æœå­é—®é¢˜ç”Ÿæˆå¤±è´¥åˆ™ä½¿ç”¨åŸå§‹é—®é¢˜ä½œä¸ºå•ä¸ªå¬å›æ­¥éª¤
        """
        try:
            sub_questions = state.get("sub_questions")
            if sub_questions:
                result = self._generate_plan_from_sub_questions(state, sub_questions)
            else:
                # å­é—®é¢˜ç”Ÿæˆå¤±è´¥æ—¶ï¼Œç›´æ¥ç”¨åŸå§‹é—®é¢˜ä½œä¸ºå•ä¸ªå¬å›æ­¥éª¤
                logger.warning("å­é—®é¢˜ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜ä½œä¸ºå¬å›æ­¥éª¤")
                result = self._generate_fallback_plan(state)
            
            yield {"type": "node_complete", "data": result}
        except Exception as e:
            logger.error(f"Error in plan_generation_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "plan_generation", "error": str(e)}
    
    def _generate_fallback_plan(self, state: AgentState) -> Dict[str, Any]:
        """
        ç”Ÿæˆ fallback è®¡åˆ’ï¼šç›´æ¥ç”¨åŸå§‹é—®é¢˜ä½œä¸ºå•ä¸ªå¬å›æ­¥éª¤
        
        Args:
            state: Agent çŠ¶æ€
            
        Returns:
            åŒ…å« planã€current_step_indexã€execution_results çš„å­—å…¸
        """
        user_query = state["user_query"]
        
        plan = {
            "locale": "zh-CN",
            "thought": "å­é—®é¢˜ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜è¿›è¡Œå¬å›",
            "title": "ä¿¡æ¯æ”¶é›†",
            "steps": [
                {
                    "title": user_query[:50] + "..." if len(user_query) > 50 else user_query,
                    "step_type": "recall",
                    "target_doc_id": None
                }
            ]
        }
        
        return {"plan": plan, "current_step_index": 0, "execution_results": []}
    
    def _generate_plan_from_sub_questions(
        self,
        state: AgentState,
        sub_questions: List[SubQuestion]
    ) -> Dict[str, Any]:
        """
        ä»å­é—®é¢˜ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        
        Args:
            state: Agent çŠ¶æ€
            sub_questions: å­é—®é¢˜åˆ—è¡¨
            
        Returns:
            åŒ…å« planã€current_step_indexã€execution_results çš„å­—å…¸
        """
        document_ids = state.get("document_ids", [])
        doc_id_map = {doc_id.lower(): doc_id for doc_id in document_ids} if document_ids else {}
        
        steps = []
        for i, sq in enumerate(sub_questions):
            target_doc_id = sq.get("target_doc_id")
            
            if target_doc_id == "" or target_doc_id == "null" or target_doc_id is None:
                target_doc_id = None
            elif document_ids:
                if not isinstance(target_doc_id, str):
                    target_doc_id = None
                elif target_doc_id in document_ids:
                    pass
                elif target_doc_id.lower() in doc_id_map:
                    target_doc_id = doc_id_map[target_doc_id.lower()]
                else:
                    target_doc_id = None
            
            step = {
                "title": sq.get("question", f"å­é—®é¢˜ {i+1}"),
                "step_type": "recall",
                "target_doc_id": target_doc_id
            }
            steps.append(step)
        
        plan = {
            "locale": "zh-CN",
            "thought": f"åŸºäºå­é—®é¢˜ç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’ï¼Œå…± {len(sub_questions)} ä¸ªå¬å›æ­¥éª¤",
            "title": "ä¿¡æ¯æ”¶é›†",
            "steps": steps
        }
        
        return {"plan": plan, "current_step_index": 0, "execution_results": []}
