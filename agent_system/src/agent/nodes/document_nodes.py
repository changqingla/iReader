"""æ–‡æ¡£ç›¸å…³èŠ‚ç‚¹"""
import asyncio
from typing import Dict, Any, AsyncGenerator, Callable, Awaitable

from langchain_core.messages import HumanMessage

from .base import BaseAgentNode
from ..state import AgentState, IntentType
from ..constants import MAX_CONCURRENT_LLM_CALLS
from ...prompts import DOCUMENT_CONDENSED_SUMMARY_PROMPT
from ...utils.logger import get_logger
from ...utils.document_summary_cache import get_document_summary_cache
from context.token_counter import calculate_tokens
from config import get_settings

logger = get_logger(__name__)

# å•ç¯‡æ–‡æ¡£æ€»ç»“çš„æœ€å¤§ token é˜ˆå€¼ï¼ˆä½¿ç”¨ direct_content_threshold é…ç½®ï¼‰
# è¶…è¿‡æ­¤é˜ˆå€¼çš„æ–‡æ¡£å°†ä½¿ç”¨å¬å›æ¨¡å¼ç”Ÿæˆæ€»ç»“


class DocumentNodes(BaseAgentNode):
    """æ–‡æ¡£æ£€æŸ¥å’Œæ€»ç»“èŠ‚ç‚¹"""
    
    @property
    def large_doc_summary_top_n(self) -> int:
        """å¤§æ–‡æ¡£æ€»ç»“æ—¶çš„å¬å›æ•°é‡ï¼ˆä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰"""
        return get_settings().large_doc_summary_top_n
    
    async def document_check_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æ–‡æ¡£æ£€æŸ¥èŠ‚ç‚¹"""
        try:
            document_ids = state.get("document_ids", [])
            doc_count = len(document_ids) if document_ids else 0
            direct_content = state.get("direct_content")
            
            if doc_count == 0 and not direct_content:
                result = {
                    "detected_intent": IntentType.GENERAL_TASK,  # æ— æ–‡æ¡£æ—¶èµ° ReAct
                    "doc_count": doc_count,
                    "route": "react",  # ç›´æ¥è®¾ç½®è·¯ç”±
                    "messages": state.get("messages", []) + [
                        HumanMessage(content=state["user_query"])
                    ]
                }
            else:
                result = {"doc_count": doc_count}
            
            yield {"type": "node_complete", "data": result}
        except Exception as e:
            logger.error(f"Error in document_check_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "document_check", "error": str(e)}

    async def document_summary_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤šæ–‡æ¡£æ€»ç»“èŠ‚ç‚¹ï¼ˆæœ‰é™å¹¶å‘ + æµå¼è¾“å‡ºï¼‰
        
        ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆå‹ç¼©æ€»ç»“ï¼Œç”¨äºåç»­çš„ç»¼åˆåˆ†æã€‚
        æ”¯æŒå®æ—¶æµå¼è¾“å‡ºæ¯ä¸ªæ–‡æ¡£çš„å¤„ç†è¿›åº¦ã€‚
        
        ç¼“å­˜æœºåˆ¶ï¼š
        - é»˜è®¤ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤ç”Ÿæˆç›¸åŒæ–‡æ¡£çš„æ€»ç»“
        - å¯é€šè¿‡ refresh_summary_cache=True å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        å¯¹äºæ¯ç¯‡æ–‡æ¡£ï¼š
        - å¦‚æœå†…å®¹å°äºé˜ˆå€¼ï¼šç›´æ¥ä½¿ç”¨å®Œæ•´å†…å®¹ç”Ÿæˆæ€»ç»“
        - å¦‚æœå†…å®¹è¶…è¿‡é˜ˆå€¼ï¼šä½¿ç”¨å¬å›è·å–å…³é”®ä¿¡æ¯åç”Ÿæˆæ€»ç»“
        
        Args:
            state: Agent çŠ¶æ€
            
        Yields:
            æµå¼äº‹ä»¶å­—å…¸ï¼ŒåŒ…æ‹¬ï¼š
            - doc_summary_start: æ–‡æ¡£å¼€å§‹å¤„ç†
            - doc_summary_chunk: æ–‡æ¡£æ€»ç»“å†…å®¹ç‰‡æ®µ
            - doc_summary_complete: æ–‡æ¡£å¤„ç†å®Œæˆ
            - thought_chunk: æ€è€ƒè¿‡ç¨‹æ–‡æœ¬
            - node_complete: èŠ‚ç‚¹å®Œæˆ
        """
        try:
            document_contents = state.get("document_contents", {})
            document_ids = state.get("document_ids", [])
            document_names = state.get("document_names", {})  # æ–‡æ¡£åç§°æ˜ å°„
            max_context_tokens = state.get("max_context_tokens", 100000)
            refresh_cache = state.get("refresh_summary_cache", False)
            
            if not document_contents and not document_ids:
                yield {"type": "node_error", "node": "document_summary", "error": "æ— æ–‡æ¡£å†…å®¹"}
                return
            
            # è¿‡æ»¤æœ‰æ•ˆæ–‡æ¡£
            valid_docs = {doc_id: document_contents.get(doc_id, "")
                         for doc_id in document_ids 
                         if document_contents.get(doc_id)}
            
            if not valid_docs:
                yield {"type": "node_error", "node": "document_summary", "error": "æ— æœ‰æ•ˆæ–‡æ¡£å†…å®¹"}
                return
            
            # è·å–ç¼“å­˜å®ä¾‹
            summary_cache = get_document_summary_cache()
            
            # æ‰¹é‡æŸ¥è¯¢ç¼“å­˜
            cached_summaries, uncached_docs = summary_cache.get_batch(
                valid_docs, 
                skip_cache=refresh_cache
            )
            
            cache_hit_count = len(cached_summaries)
            cache_miss_count = len(uncached_docs)
            total_docs = len(valid_docs)
            
            # å‘é€æ–‡æ¡£æ€»ç»“åˆå§‹åŒ–äº‹ä»¶
            yield {
                "type": "doc_summary_init",
                "data": {
                    "total": total_docs,
                    "cached": cache_hit_count,
                    "to_generate": cache_miss_count
                }
            }
            
            # ä¸è¾“å‡ºå¤„ç†çŠ¶æ€ï¼Œæ–‡æ¡£è¿›åº¦é€šè¿‡ doc_summary_* äº‹ä»¶å±•ç¤º
            
            # è¯¦ç»†çš„ç¼“å­˜æ—¥å¿—
            logger.info("=" * 60)
            logger.info(f"ğŸ“¦ æ–‡æ¡£æ€»ç»“ç¼“å­˜çŠ¶æ€:")
            logger.info(f"   - æ€»æ–‡æ¡£æ•°: {total_docs}")
            logger.info(f"   - ç¼“å­˜å‘½ä¸­: {cache_hit_count}")
            logger.info(f"   - ç¼“å­˜æœªå‘½ä¸­: {cache_miss_count}")
            logger.info(f"   - å¼ºåˆ¶åˆ·æ–°: {refresh_cache}")
            if cached_summaries:
                logger.info(f"   - å‘½ä¸­çš„æ–‡æ¡£ID: {list(cached_summaries.keys())[:5]}{'...' if len(cached_summaries) > 5 else ''}")
            if uncached_docs:
                logger.info(f"   - æœªå‘½ä¸­çš„æ–‡æ¡£ID: {list(uncached_docs.keys())[:5]}{'...' if len(uncached_docs) > 5 else ''}")
            logger.info("=" * 60)
            
            # ä¸ºç¼“å­˜å‘½ä¸­çš„æ–‡æ¡£å‘é€å®Œæˆäº‹ä»¶
            cached_index = 0
            for doc_id, summary in cached_summaries.items():
                doc_name = document_names.get(doc_id, f"æ–‡æ¡£ {cached_index + 1}")
                yield {
                    "type": "doc_summary_complete",
                    "data": {
                        "doc_id": doc_id,
                        "doc_name": doc_name,
                        "summary": summary,
                        "from_cache": True,
                        "index": cached_index,
                        "total": total_docs
                    }
                }
                cached_index += 1
            
            # å¦‚æœæ‰€æœ‰æ–‡æ¡£éƒ½å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›
            if cache_miss_count == 0 and not refresh_cache:
                logger.info(f"âœ… æ‰€æœ‰ {cache_hit_count} ç¯‡æ–‡æ¡£æ€»ç»“å·²ä»ç¼“å­˜åŠ è½½ï¼Œè·³è¿‡ LLM è°ƒç”¨")
                yield {
                    "type": "node_complete",
                    "data": {"document_summaries": cached_summaries}
                }
                return
            
            # è®¡ç®—å•ç¯‡æ–‡æ¡£çš„ token é˜ˆå€¼
            settings = get_settings()
            single_doc_threshold = int(max_context_tokens * settings.direct_content_threshold)
            
            # åˆ†ç±»éœ€è¦ç”Ÿæˆçš„æ–‡æ¡£ï¼šå°æ–‡æ¡£ç›´æ¥å¤„ç†ï¼Œå¤§æ–‡æ¡£éœ€è¦å¬å›
            small_docs = []
            large_docs = []
            
            docs_to_process = uncached_docs if not refresh_cache else valid_docs
            
            for doc_id, content in docs_to_process.items():
                token_count = calculate_tokens(content)
                if token_count <= single_doc_threshold:
                    small_docs.append((doc_id, content, token_count))
                    logger.info(f"æ–‡æ¡£ {doc_id}: {token_count:,} tokens (ç›´æ¥å¤„ç†)")
                else:
                    large_docs.append((doc_id, content, token_count))
                    logger.info(f"æ–‡æ¡£ {doc_id}: {token_count:,} tokens (éœ€è¦å¬å›ï¼Œé˜ˆå€¼: {single_doc_threshold:,})")
            
            # ä¸è¾“å‡ºæ–‡æ¡£åˆ†ç±»ä¿¡æ¯
            
            # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘ LLM è°ƒç”¨æ•°
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM_CALLS)
            docs_to_generate = len(small_docs) + len(large_docs)
            
            logger.info(f"ğŸ“Š å¼€å§‹å¹¶è¡Œå¤„ç† {docs_to_generate} ç¯‡æ–‡æ¡£ï¼Œæœ€å¤§å¹¶å‘æ•°: {MAX_CONCURRENT_LLM_CALLS}")
            
            # åˆ›å»ºäº‹ä»¶é˜Ÿåˆ—ç”¨äºæµå¼è¾“å‡º
            event_queue: asyncio.Queue = asyncio.Queue()
            
            async def summarize_with_progress(doc_id: str, content: str, is_large: bool, index: int):
                """å¸¦è¿›åº¦è¾“å‡ºçš„æ–‡æ¡£æ€»ç»“ï¼ˆå¤§æ–‡æ¡£å’Œå°æ–‡æ¡£éƒ½æ”¯æŒæµå¼è¾“å‡ºï¼‰"""
                doc_name = document_names.get(doc_id, f"æ–‡æ¡£ {index + 1}")
                
                async with semaphore:
                    # å‘é€å¼€å§‹äº‹ä»¶
                    await event_queue.put({
                        "type": "doc_summary_start",
                        "data": {
                            "doc_id": doc_id,
                            "doc_name": doc_name,
                            "index": index,
                            "total": total_docs
                        }
                    })
                    
                    try:
                        # å®šä¹‰æµå¼è¾“å‡ºå›è°ƒ
                        async def on_chunk(chunk_content: str):
                            await event_queue.put({
                                "type": "doc_summary_chunk",
                                "data": {
                                    "doc_id": doc_id,
                                    "content": chunk_content
                                }
                            })
                        
                        if is_large:
                            # å¤§æ–‡æ¡£ä½¿ç”¨å¬å›æ¨¡å¼ï¼ˆç°åœ¨ä¹Ÿæ”¯æŒæµå¼è¾“å‡ºï¼‰
                            summary = await self._summarize_large_document_with_recall_stream(
                                doc_id, state, on_chunk
                            )
                        else:
                            # å°æ–‡æ¡£æµå¼ç”Ÿæˆæ€»ç»“
                            summary = ""
                            prompt = DOCUMENT_CONDENSED_SUMMARY_PROMPT.format(document_content=content)
                            
                            chunk_count = 0
                            logger.info(f"ğŸ“ æ–‡æ¡£ {doc_id} å¼€å§‹æµå¼ç”Ÿæˆæ€»ç»“...")
                            try:
                                async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                                    #logger.info(f"ğŸ“ æ–‡æ¡£ {doc_id} æ”¶åˆ°åŸå§‹ chunk: type={type(chunk)}, chunk={str(chunk)[:100]}")
                                    chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                                    summary += chunk_content
                                    chunk_count += 1
                                    #if chunk_count <= 5:
                                        #logger.info(f"ğŸ“ æ–‡æ¡£ {doc_id} æ”¶åˆ° chunk #{chunk_count}, é•¿åº¦: {len(chunk_content)}, å†…å®¹: {chunk_content[:50]}")
                                    # å‘é€æµå¼å†…å®¹
                                    await on_chunk(chunk_content)
                                logger.info(f"âœ… æ–‡æ¡£ {doc_id} æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {chunk_count} ä¸ª chunkï¼Œæ€»é•¿åº¦: {len(summary)}")
                            except Exception as e:
                                logger.error(f"âŒ æ–‡æ¡£ {doc_id} æµå¼ç”Ÿæˆå¼‚å¸¸: {e}", exc_info=True)
                                raise
                        
                        # å‘é€å®Œæˆäº‹ä»¶
                        await event_queue.put({
                            "type": "doc_summary_complete",
                            "data": {
                                "doc_id": doc_id,
                                "doc_name": doc_name,
                                "summary": summary,
                                "from_cache": False,
                                "index": index,
                                "total": total_docs
                            }
                        })
                        
                        return doc_id, summary, content
                        
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£ {doc_id} æ€»ç»“å¤±è´¥: {e}")
                        await event_queue.put({
                            "type": "doc_summary_error",
                            "data": {
                                "doc_id": doc_id,
                                "doc_name": doc_name,
                                "error": str(e)
                            }
                        })
                        return doc_id, f"[æ–‡æ¡£æ€»ç»“å¤±è´¥: {str(e)}]", content
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = []
            current_index = cache_hit_count  # ä»ç¼“å­˜å‘½ä¸­æ•°å¼€å§‹è®¡æ•°
            
            # å°æ–‡æ¡£ä»»åŠ¡
            for doc_id, content, _ in small_docs:
                tasks.append(summarize_with_progress(doc_id, content, is_large=False, index=current_index))
                current_index += 1
            
            # å¤§æ–‡æ¡£ä»»åŠ¡
            for doc_id, content, _ in large_docs:
                tasks.append(summarize_with_progress(doc_id, content, is_large=True, index=current_index))
                current_index += 1
            
            # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡ï¼ŒåŒæ—¶æ¶ˆè´¹äº‹ä»¶é˜Ÿåˆ—
            async def run_tasks():
                results = await asyncio.gather(*tasks, return_exceptions=True)
                await event_queue.put(None)  # å‘é€ç»“æŸä¿¡å·
                return results
            
            # å¯åŠ¨ä»»åŠ¡
            task_runner = asyncio.create_task(run_tasks())
            
            # æ¶ˆè´¹äº‹ä»¶é˜Ÿåˆ—å¹¶ yield
            while True:
                event = await event_queue.get()
                if event is None:
                    break
                # ğŸ” Debug: Log event being yielded
                # if event.get("type") == "doc_summary_chunk":
                #     logger.info(f"ğŸ“¤ [Queue] Yielding doc_summary_chunk: doc_id={event.get('data', {}).get('doc_id')}, content_len={len(event.get('data', {}).get('content', ''))}")
                # 
                yield event
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶è·å–ç»“æœ
            results = await task_runner
            
            # å¤„ç†ç»“æœå¹¶å­˜å…¥ç¼“å­˜
            new_summaries = {}
            docs_for_cache = {}
            
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"æ–‡æ¡£æ€»ç»“å¤±è´¥: {result}")
                else:
                    doc_id, summary, content = result
                    new_summaries[doc_id] = summary
                    docs_for_cache[doc_id] = content
                    logger.info(f"æ–‡æ¡£ {doc_id} æ€»ç»“å®Œæˆï¼Œé•¿åº¦: {len(summary)}")
            
            # æ‰¹é‡å­˜å…¥ç¼“å­˜
            if new_summaries:
                saved_count = summary_cache.set_batch(docs_for_cache, new_summaries)
                logger.info(f"ğŸ’¾ å·²å°† {saved_count} ç¯‡æ–°ç”Ÿæˆçš„æ–‡æ¡£æ€»ç»“å­˜å…¥ç¼“å­˜")
            
            # åˆå¹¶ç¼“å­˜å‘½ä¸­å’Œæ–°ç”Ÿæˆçš„æ€»ç»“
            document_summaries = {**cached_summaries, **new_summaries}
            
            yield {
                "type": "node_complete",
                "data": {"document_summaries": document_summaries}
            }
            
        except Exception as e:
            logger.error(f"Error in document_summary_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "document_summary", "error": str(e)}
    
    async def _summarize_large_document_with_recall_stream(
        self,
        doc_id: str,
        state: AgentState,
        on_chunk: Callable[[str], Awaitable[None]]
    ) -> str:
        """
        ä½¿ç”¨å¬å›æ¨¡å¼ç”Ÿæˆå¤§æ–‡æ¡£çš„æ€»ç»“ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼‰
        
        å¯¹äºè¶…è¿‡é˜ˆå€¼çš„å¤§æ–‡æ¡£ï¼Œå…ˆé€šè¿‡å¬å›è·å–å…³é”®ä¿¡æ¯ï¼Œå†æµå¼ç”Ÿæˆæ€»ç»“ã€‚
        ä½¿ç”¨æ›´å¤§çš„ top_n (35) ä»¥è¦†ç›–æ–‡æ¡£çš„æ›´å¤šå†…å®¹ã€‚
        
        Args:
            doc_id: æ–‡æ¡£ID
            state: Agent çŠ¶æ€ï¼ˆç”¨äºè·å–å¬å›å·¥å…·é…ç½®ï¼‰
            on_chunk: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°
            
        Returns:
            æ–‡æ¡£æ€»ç»“æ–‡æœ¬
        """
        from ...tools import create_recall_tool
        
        # ä¸ºå¤§æ–‡æ¡£æ€»ç»“åˆ›å»ºä¸“ç”¨çš„å¬å›å·¥å…·ï¼Œä½¿ç”¨æ›´å¤§çš„ top_n
        large_doc_recall_tool = create_recall_tool(
            api_url=self.recall_tool.api_url,
            index_names=self.recall_tool.index_names,
            es_host=self.recall_tool.es_host,
            model_base_url=self.recall_tool.model_base_url,
            api_key=self.recall_tool.api_key,
            doc_ids=[doc_id],
            top_n=self.large_doc_summary_top_n,  # ä»é…ç½®æ–‡ä»¶åŠ è½½
            similarity_threshold=self.recall_tool.similarity_threshold,
            vector_similarity_weight=self.recall_tool.vector_similarity_weight,
            model_factory=self.recall_tool.model_factory,
            model_name=self.recall_tool.model_name,
            use_rerank=self.recall_tool.use_rerank,
            rerank_factory=self.recall_tool.rerank_factory,
            rerank_model_name=self.recall_tool.rerank_model_name,
            rerank_base_url=self.recall_tool.rerank_base_url,
            rerank_api_key=self.recall_tool.rerank_api_key
        )
        
        # ä½¿ç”¨é€šç”¨æŸ¥è¯¢è·å–æ–‡æ¡£çš„å…³é”®ä¿¡æ¯
        try:
            logger.info(f"ğŸ“š å¤§æ–‡æ¡£ {doc_id} å¼€å§‹å¬å›ï¼Œtop_n={self.large_doc_summary_top_n}")
            recalled_content = await large_doc_recall_tool._arun(
                "æ€»ç»“è¿™ç¯‡æ–‡çŒ®çš„ä¸»è¦å†…å®¹"
            )
        except Exception as e:
            logger.error(f"æ–‡æ¡£ {doc_id} å¬å›å¤±è´¥: {e}")
            return f"[æ–‡æ¡£ {doc_id} æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼šæ— æ³•è·å–æ–‡æ¡£å†…å®¹]"
        
        if not recalled_content or not recalled_content.strip():
            logger.warning(f"æ–‡æ¡£ {doc_id} å¬å›ç»“æœä¸ºç©º")
            return f"[æ–‡æ¡£ {doc_id} æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼šå¬å›ç»“æœä¸ºç©º]"
        
        logger.info(f"æ–‡æ¡£ {doc_id} å¬å›å†…å®¹é•¿åº¦: {len(recalled_content)} å­—ç¬¦")
        
        # ä½¿ç”¨å¬å›å†…å®¹æµå¼ç”Ÿæˆæ€»ç»“
        prompt = DOCUMENT_CONDENSED_SUMMARY_PROMPT.format(document_content=recalled_content)
        
        summary = ""
        chunk_count = 0
        logger.info(f"ğŸ“ å¤§æ–‡æ¡£ {doc_id} å¼€å§‹æµå¼ç”Ÿæˆæ€»ç»“...")
        async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
            chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
            summary += chunk_content
            chunk_count += 1
            # if chunk_count <= 3 or chunk_count % 50 == 0:
            #     logger.info(f"ğŸ“ å¤§æ–‡æ¡£ {doc_id} æ”¶åˆ° chunk #{chunk_count}, é•¿åº¦: {len(chunk_content)}")
            # å‘é€æµå¼å†…å®¹
            await on_chunk(chunk_content)
        
        #logger.info(f"âœ… å¤§æ–‡æ¡£ {doc_id} æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {chunk_count} ä¸ª chunkï¼Œæ€»é•¿åº¦: {len(summary)}")
        return summary
