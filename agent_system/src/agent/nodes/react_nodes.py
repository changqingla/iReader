"""ReAct Agent èŠ‚ç‚¹"""
import asyncio
from typing import Dict, Any, AsyncGenerator, Optional, List, TYPE_CHECKING

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import BaseTool

from .base import BaseAgentNode
from ..state import AgentState
from ..react import (
    ReActConfig, 
    Scratchpad, 
    ScratchpadEntry, 
    ActionParser,
    create_default_hook_manager,
    HookAction,
    CompletionDetector,
    CompletionReason,
)
from ...prompts import REACT_AGENT_PROMPT
from ...utils.logger import get_logger
from ...tools.registry import get_tool_registry

if TYPE_CHECKING:
    from ...mcp.tool_adapter import MCPToolAdapter

logger = get_logger(__name__)


class ReActNodes(BaseAgentNode):
    """ReAct Agent èŠ‚ç‚¹ - æ”¯æŒ Hook æœºåˆ¶å’Œæ™ºèƒ½å®Œæˆæ£€æµ‹"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.config = ReActConfig()
        
        # åŠ¨æ€æ›´æ–°å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆåŒ…å« MCP å·¥å…·ï¼‰
        self._update_available_tools()
        
        self.action_parser = ActionParser(self.config)
        
        # åˆå§‹åŒ– Hook ç®¡ç†å™¨
        self.hook_manager = create_default_hook_manager() if self.config.enable_hooks else None
        
        # åˆå§‹åŒ–å®Œæˆæ£€æµ‹å™¨
        self.completion_detector = CompletionDetector(self.config) if self.config.enable_completion_detection else None
    
    def _update_available_tools(self) -> None:
        """åŠ¨æ€æ›´æ–°å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ŒåŒ…å« MCP å·¥å…·"""
        # åŸºç¡€å·¥å…·
        tools = ["recall", "web_search"]
        
        # æ·»åŠ  MCP å·¥å…·
        registry = get_tool_registry()
        for tool in registry.get_mcp_tools():
            tools.append(tool.name)
        
        # finish å§‹ç»ˆåœ¨æœ€å
        tools.append("finish")
        
        # æ›´æ–°é…ç½®
        self.config.available_tools = tuple(tools)
        logger.info(f"ğŸ“Œ ReAct å¯ç”¨å·¥å…·: {', '.join(tools)}")
    
    def _get_available_tools_description(self) -> str:
        """
        è·å–æ‰€æœ‰å¯ç”¨å·¥å…·çš„æè¿°ï¼Œç”¨äº ReAct prompt
        
        Returns:
            æ ¼å¼åŒ–çš„å·¥å…·æè¿°å­—ç¬¦ä¸²
        """
        tools_desc = []
        
        # 1. å†…ç½®å·¥å…·
        tools_desc.append('1. **recall(query)**: Search the user\'s document library for relevant information. Use this when you need to find specific content from uploaded documents.')
        tools_desc.append('2. **web_search(query)**: Search the internet for external information. Use this when you need up-to-date information or knowledge not in the documents.')
        
        # 2. MCP å·¥å…·ï¼ˆä» ToolRegistry è·å–ï¼‰
        registry = get_tool_registry()
        mcp_tools = registry.get_mcp_tools()
        
        tool_num = 3  # ä» 3 å¼€å§‹ç¼–å·ï¼ˆ1 å’Œ 2 æ˜¯å†…ç½®å·¥å…·ï¼‰
        for tool in mcp_tools:
            # æ ¼å¼åŒ–å·¥å…·æè¿°
            tool_desc = f'{tool_num}. **{tool.name}(...)**: {tool.description}'
            tools_desc.append(tool_desc)
            tool_num += 1
        
        # 3. finish å·¥å…·ï¼ˆå§‹ç»ˆæœ€åï¼‰
        tools_desc.append(f'{tool_num}. **finish(answer)**: Complete the task and provide the final answer. Use this when you have gathered enough information to answer the user\'s question.')
        
        return '\n'.join(tools_desc)
    
    def _get_available_tool_names(self) -> List[str]:
        """
        è·å–æ‰€æœ‰å¯ç”¨å·¥å…·çš„åç§°åˆ—è¡¨
        
        Returns:
            å·¥å…·åç§°åˆ—è¡¨
        """
        tool_names = ['recall', 'web_search']
        
        # æ·»åŠ  MCP å·¥å…·åç§°
        registry = get_tool_registry()
        for tool in registry.get_mcp_tools():
            tool_names.append(tool.name)
        
        tool_names.append('finish')
        return tool_names
    
    async def react_agent_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ReAct Agent ä¸»å¾ªç¯
        
        æ‰§è¡Œ Thought â†’ Action â†’ Observation å¾ªç¯ï¼Œç›´åˆ°ï¼š
        1. Agent è°ƒç”¨ finish å·¥å…·
        2. è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        3. æ™ºèƒ½æ£€æµ‹åˆ°åº”è¯¥ç»“æŸ
        4. å‘ç”Ÿé”™è¯¯
        
        Args:
            state: Agent çŠ¶æ€
            
        Yields:
            æµå¼äº‹ä»¶å­—å…¸
        """
        try:
            user_query = state["user_query"]
            session_id = state.get("session_id")
            document_ids = state.get("document_ids", [])
            
            # åˆå§‹åŒ– scratchpad
            scratchpad = Scratchpad(
                max_tokens=self.config.max_scratchpad_tokens,
                model=self.llm.model_name if hasattr(self.llm, 'model_name') else "gpt-4"
            )
            
            # è·å–å¯¹è¯å†å²
            context_str = await self._get_conversation_context_async(state, stage="simple_interaction")
            
            # æ„å»ºæ–‡æ¡£ä¿¡æ¯
            document_info = self._build_document_info(state)
            
            iteration = 0
            final_answer = ""
            
            while iteration < self.config.max_iterations:
                iteration += 1
                logger.info(f"ğŸ”„ ReAct è¿­ä»£ {iteration}/{self.config.max_iterations}")
                
                # æ™ºèƒ½å®Œæˆæ£€æµ‹ï¼ˆä¼˜åŒ– 5ï¼‰
                if self.completion_detector and iteration > 1:
                    completion_result = self.completion_detector.check(scratchpad, user_query)
                    if completion_result.should_finish:
                        logger.info(f"ğŸ¯ æ™ºèƒ½æ£€æµ‹å»ºè®®ç»“æŸ: {completion_result.reason.value}")
                        # å®Œæˆæ£€æµ‹æç¤ºåªè®°å½•æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°å‰ç«¯
                        # å¯¹äºæŸäº›åŸå› ï¼Œå¼ºåˆ¶ç»“æŸ
                        if completion_result.reason in (
                            CompletionReason.STUCK_IN_LOOP,
                            CompletionReason.MAX_ERRORS,
                            CompletionReason.TOKEN_LIMIT
                        ):
                            final_answer = self._generate_forced_answer(scratchpad, user_query)
                            break
                
                # æ„å»º promptï¼ˆåŒ…å«åŠ¨æ€å·¥å…·åˆ—è¡¨å’Œå½“å‰æ—¥æœŸï¼‰
                available_tools = self._get_available_tools_description()
                tool_names = ', '.join(self._get_available_tool_names())
                
                # è·å–å½“å‰æ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
                from datetime import datetime, timezone, timedelta
                beijing_tz = timezone(timedelta(hours=8))
                current_date = datetime.now(beijing_tz).strftime("%Y-%m-%d")
                
                prompt = REACT_AGENT_PROMPT.format(
                    user_query=user_query,
                    conversation_history=context_str if context_str else "æ— å†å²å¯¹è¯",
                    document_info=document_info,
                    scratchpad=scratchpad.to_string() if len(scratchpad) > 0 else "ï¼ˆé¦–æ¬¡æ€è€ƒï¼Œæ— å†å²è®°å½•ï¼‰",
                    available_tools=available_tools,
                    tool_names=tool_names,
                    current_date=current_date
                )
                
                # è°ƒç”¨ LLM - æµå¼è¾“å‡ºæ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
                # å…³é”®æ”¹è¿›ï¼š
                # 1. æ€è€ƒè¿‡ç¨‹è¾“å‡ºåˆ° thought_chunk
                # 2. ä½¿ç”¨ find è€Œä¸æ˜¯ rfindï¼Œç¡®ä¿åªå¤„ç†ç¬¬ä¸€ä¸ª finish
                # 3. ä¸€æ—¦æ£€æµ‹åˆ° finishï¼Œåªè¾“å‡ºè¯¥ finish çš„ç­”æ¡ˆï¼Œå¿½ç•¥åç»­å†…å®¹
                llm_output = ""
                thought_output_started = False  # æ˜¯å¦å·²å¼€å§‹è¾“å‡ºæ€è€ƒå†…å®¹
                thought_output_pos = 0  # æ€è€ƒå†…å®¹å·²è¾“å‡ºåˆ°çš„ä½ç½®
                answer_streaming = False  # æ˜¯å¦åœ¨è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
                answer_output_started = False  # æ˜¯å¦å·²å¼€å§‹è¾“å‡ºç­”æ¡ˆ
                answer_output_pos = 0  # ç­”æ¡ˆå·²è¾“å‡ºåˆ°çš„ä½ç½®
                first_finish_pos = -1  # ç¬¬ä¸€ä¸ª finish çš„ä½ç½®ï¼ˆä¸€æ—¦ç¡®å®šå°±ä¸å˜ï¼‰
                first_action_input_pos = -1  # ç¬¬ä¸€ä¸ª finish å¯¹åº”çš„ action input ä½ç½®
                
                async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                    chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                    llm_output += chunk_content
                    llm_lower = llm_output.lower()
                    
                    # æ£€æŸ¥æ˜¯å¦è¿›å…¥ç­”æ¡ˆè¾“å‡ºé˜¶æ®µï¼ˆæ£€æµ‹åˆ°ç¬¬ä¸€ä¸ª finish åŠ¨ä½œï¼‰
                    # å…³é”®ï¼šä½¿ç”¨ find è€Œä¸æ˜¯ rfindï¼Œåªå¤„ç†ç¬¬ä¸€ä¸ª finish
                    if not answer_streaming:
                        for finish_marker in ["action: finish", "action:finish"]:
                            pos = llm_lower.find(finish_marker)
                            if pos != -1:
                                # æ£€æŸ¥ finish åé¢æ˜¯å¦æœ‰ action input
                                remaining = llm_lower[pos:]
                                if "action input:" in remaining or "action_input:" in remaining:
                                    answer_streaming = True
                                    first_finish_pos = pos
                                    break
                    
                    # å¦‚æœå·²ç»åœ¨è¾“å‡ºç­”æ¡ˆé˜¶æ®µ
                    if answer_streaming:
                        # ä½¿ç”¨å·²è®°å½•çš„ç¬¬ä¸€ä¸ª finish ä½ç½®
                        if first_finish_pos == -1:
                            continue
                        
                        # å¦‚æœè¿˜æ²¡æ‰¾åˆ° action input ä½ç½®ï¼Œç°åœ¨æ‰¾
                        if first_action_input_pos == -1:
                            for marker in ["action input:", "action_input:"]:
                                marker_pos = llm_lower.find(marker, first_finish_pos)
                                if marker_pos != -1:
                                    first_action_input_pos = marker_pos + len(marker)
                                    # è·³è¿‡ç©ºç™½
                                    while first_action_input_pos < len(llm_output) and llm_output[first_action_input_pos] in ' \t\n':
                                        first_action_input_pos += 1
                                    break
                        
                        if first_action_input_pos == -1:
                            continue
                        
                        # ç¡®å®šç­”æ¡ˆçš„ç»“æŸä½ç½®ï¼ˆé‡åˆ°ä¸‹ä¸€ä¸ª Thought: æˆ– Action: å°±åœæ­¢ï¼‰
                        answer_end = len(llm_output)
                        for stop_marker in ["\nthought:", "\naction:", "\nobservation:"]:
                            stop_pos = llm_lower.find(stop_marker, first_action_input_pos)
                            if stop_pos != -1 and stop_pos < answer_end:
                                answer_end = stop_pos
                        
                        # è¾“å‡ºæ–°å¢çš„ç­”æ¡ˆå†…å®¹
                        if not answer_output_started:
                            answer_output_pos = first_action_input_pos
                            answer_output_started = True
                        
                        # åªè¾“å‡ºåˆ°ç­”æ¡ˆç»“æŸä½ç½®
                        if answer_end > answer_output_pos:
                            new_content = llm_output[answer_output_pos:answer_end]
                            if new_content:
                                yield {"type": "answer_chunk", "data": {"content": new_content}}
                            answer_output_pos = answer_end
                        continue
                    
                    # æµå¼è¾“å‡ºæ€è€ƒéƒ¨åˆ†ï¼ˆåªåœ¨é finish é˜¶æ®µï¼‰
                    # æ£€æŸ¥æ˜¯å¦é‡åˆ° Action: æ ‡è®°
                    action_pos = -1
                    for marker in ["\naction:", "\nAction:", "\nACTION:"]:
                        pos = llm_lower.find(marker.lower())
                        if pos != -1:
                            action_pos = pos
                            break
                    
                    # ç¡®å®šæ€è€ƒå†…å®¹çš„èµ·å§‹ä½ç½®ï¼ˆè·³è¿‡ Thought: å‰ç¼€ï¼‰
                    if not thought_output_started:
                        thought_start = 0
                        for prefix in ["thought:", "Thought:", "THOUGHT:", "æ€è€ƒ:"]:
                            prefix_lower = prefix.lower()
                            if llm_lower.strip().startswith(prefix_lower):
                                prefix_pos = llm_lower.find(prefix_lower)
                                thought_start = prefix_pos + len(prefix)
                                # è·³è¿‡å‰ç¼€åçš„ç©ºç™½
                                while thought_start < len(llm_output) and llm_output[thought_start] in ' \t':
                                    thought_start += 1
                                break
                        
                        # åªæœ‰å½“æˆ‘ä»¬ç¡®å®šäº†å‰ç¼€ä½ç½®åæ‰å¼€å§‹è¾“å‡º
                        if thought_start > 0 or len(llm_output) > 20:
                            thought_output_started = True
                            thought_output_pos = thought_start
                    
                    if thought_output_started:
                        # ç¡®å®šæœ¬æ¬¡è¾“å‡ºçš„ç»“æŸä½ç½®
                        end_pos = action_pos if action_pos != -1 else len(llm_output)
                        
                        # è¾“å‡ºæ–°å¢çš„æ€è€ƒå†…å®¹
                        if end_pos > thought_output_pos:
                            new_content = llm_output[thought_output_pos:end_pos]
                            if new_content:
                                yield {
                                    "type": "thought_chunk",
                                    "data": {
                                        "content": new_content,
                                        "phase": "thinking"
                                    }
                                }
                            thought_output_pos = end_pos
                
                # æ€è€ƒéƒ¨åˆ†ç»“æŸï¼Œæ·»åŠ æ¢è¡Œï¼ˆåªåœ¨é finish æƒ…å†µä¸‹ï¼‰
                if not answer_streaming:
                    yield {
                        "type": "thought_chunk",
                        "data": {
                            "content": "\n\n",
                            "phase": "thinking"
                        }
                    }
                
                # è§£æå®Œæ•´è¾“å‡ºè·å– Action
                parsed = self.action_parser.parse(llm_output)
                
                # å¤„ç†æ— æ•ˆ Action
                if not parsed.is_valid:
                    logger.warning(f"âš ï¸ æ— æ•ˆ Action: {parsed.error_message}")
                    observation = f"[ERROR] {parsed.error_message}"
                    
                    # æ·»åŠ åˆ° scratchpad
                    entry = ScratchpadEntry(
                        thought=parsed.thought or "ï¼ˆè§£æå¤±è´¥ï¼‰",
                        action=parsed.action or "unknown",
                        action_input=parsed.action_input or "",
                        observation=observation
                    )
                    scratchpad.add_entry(entry)
                    
                    # é”™è¯¯ä¿¡æ¯åªè®°å½•æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°å‰ç«¯
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ finish
                if self.action_parser.is_finish_action(parsed):
                    final_answer = self.action_parser.extract_final_answer(parsed)
                    logger.info(f"âœ… ReAct å®Œæˆï¼Œè¿­ä»£æ¬¡æ•°: {iteration}")
                    break
                
                # Hook å‰å¤„ç†
                action = parsed.action
                action_input = parsed.action_input
                
                if self.hook_manager:
                    action, action_input, skip_message = await self.hook_manager.run_pre_hooks(
                        action, action_input, state
                    )
                    if skip_message:
                        logger.info(f"â­ï¸ Hook è·³è¿‡å·¥å…·è°ƒç”¨: {skip_message}")
                        observation = f"[SKIPPED] {skip_message}"
                        entry = ScratchpadEntry(
                            thought=parsed.thought,
                            action=action,
                            action_input=action_input,
                            observation=observation
                        )
                        scratchpad.add_entry(entry)
                        # è·³è¿‡ä¿¡æ¯åªè®°å½•æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°å‰ç«¯
                        continue
                
                # æ‰§è¡Œå·¥å…·ï¼ˆä¸è¾“å‡ºåŠ¨ä½œä¿¡æ¯åˆ°å‰ç«¯ï¼‰
                observation = await self._execute_tool(action, action_input, state)
                
                # Hook åå¤„ç†ï¼ˆä¼˜åŒ– 1ï¼‰
                if self.hook_manager:
                    observation = await self.hook_manager.run_post_hooks(
                        action, action_input, observation, state
                    )
                
                # æ·»åŠ åˆ° scratchpad
                entry = ScratchpadEntry(
                    thought=parsed.thought,
                    action=action,
                    action_input=action_input,
                    observation=observation
                )
                scratchpad.add_entry(entry)
                # è§‚å¯Ÿç»“æœå’Œç»Ÿè®¡ä¿¡æ¯åªè®°å½•æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°å‰ç«¯
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ä½†æ²¡æœ‰ finish
            if not final_answer:
                logger.warning(f"âš ï¸ ReAct è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {self.config.max_iterations}ï¼Œå¼ºåˆ¶ç»“æŸ")
                final_answer = self._generate_forced_answer(scratchpad, user_query)
                # æœ€å¤§è¿­ä»£æç¤ºåªè®°å½•æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°å‰ç«¯
                # å¼ºåˆ¶ç”Ÿæˆçš„ç­”æ¡ˆéœ€è¦æµå¼è¾“å‡º
                for chunk in self._chunk_text(final_answer, chunk_size=50):
                    yield {"type": "answer_chunk", "data": {"content": chunk}}
            
            # ä¿å­˜ä¼šè¯
            if session_id:
                model_name = self.llm.model_name if hasattr(self.llm, 'model_name') else "unknown"
                
                if not state.get("_user_message_saved"):
                    await asyncio.to_thread(
                        self.session_manager.add_user_message,
                        session_id=session_id,
                        content=user_query,
                        model_name=model_name
                    )
                
                await asyncio.to_thread(
                    self.session_manager.add_assistant_message,
                    session_id=session_id,
                    content=final_answer,
                    model_name=model_name
                )
            
            # æ³¨æ„ï¼šæœ€ç»ˆç­”æ¡ˆå·²åœ¨ LLM æµå¼è¾“å‡ºæ—¶è¾“å‡ºï¼Œè¿™é‡Œä¸å†é‡å¤è¾“å‡º
            
            # è¿”å›ç»“æœ
            result = {
                "final_answer": final_answer,
                "react_iteration": iteration,
                "messages": state.get("messages", []) + [AIMessage(content=final_answer)]
            }
            
            yield {"type": "node_complete", "data": result}
            
        except Exception as e:
            logger.error(f"Error in react_agent_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "react_agent", "error": str(e)}
    
    def _build_document_info(self, state: AgentState) -> str:
        """æ„å»ºæ–‡æ¡£ä¿¡æ¯å­—ç¬¦ä¸²"""
        document_ids = state.get("document_ids", [])
        document_names = state.get("document_names", {}) or {}
        
        if not document_ids:
            return "æ— å…³è”æ–‡æ¡£"
        
        lines = [f"å…± {len(document_ids)} ä¸ªæ–‡æ¡£:"]
        for doc_id in document_ids:
            doc_name = document_names.get(doc_id, doc_id)
            lines.append(f"  - {doc_name}")
        
        return "\n".join(lines)
    
    async def _execute_tool(
        self,
        action: str,
        action_input: str,
        state: AgentState
    ) -> str:
        """
        æ‰§è¡Œå·¥å…·è°ƒç”¨
        
        æ”¯æŒä¸‰ç§ç±»å‹çš„å·¥å…·ï¼š
        1. å†…ç½®å·¥å…·ï¼ˆrecall, web_searchï¼‰
        2. MCP å·¥å…·ï¼ˆé€šè¿‡ ToolRegistry æ³¨å†Œï¼‰
        3. æœªçŸ¥å·¥å…·ï¼ˆè¿”å›é”™è¯¯ï¼‰
        
        Args:
            action: å·¥å…·åç§°
            action_input: å·¥å…·è¾“å…¥
            state: Agent çŠ¶æ€
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœï¼ˆObservationï¼‰
        """
        try:
            # 1. æ£€æŸ¥å†…ç½®å·¥å…·
            if action == "recall":
                return await self._execute_recall(action_input, state)
            elif action == "web_search":
                return await self._execute_web_search(action_input)
            
            # 2. æ£€æŸ¥ MCP å·¥å…·ï¼ˆé€šè¿‡ ToolRegistryï¼‰
            registry = get_tool_registry()
            if registry.has_tool(action):
                return await self._execute_mcp_tool(action, action_input)
            
            # 3. æœªçŸ¥å·¥å…·
            return f"[ERROR] Unknown tool: {action}"
        except asyncio.TimeoutError:
            return f"[ERROR] Tool execution timed out after {self.config.tool_timeout}s"
        except Exception as e:
            logger.error(f"Tool execution error: {str(e)}", exc_info=True)
            return f"[ERROR] Tool execution failed: {str(e)}"
    
    async def _execute_mcp_tool(self, tool_name: str, tool_input: str) -> str:
        """
        æ‰§è¡Œ MCP å·¥å…·è°ƒç”¨
        
        Args:
            tool_name: MCP å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥ï¼ˆå­—ç¬¦ä¸²ï¼Œå°†è¢«è§£æä¸ºå‚æ•°ï¼‰
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            registry = get_tool_registry()
            tool = registry.get_tool(tool_name)
            
            if tool is None:
                return f"[ERROR] MCP tool '{tool_name}' not found"
            
            # è§£æè¾“å…¥å‚æ•°
            # MCP å·¥å…·å¯èƒ½éœ€è¦ JSON æ ¼å¼çš„è¾“å…¥ï¼Œæˆ–è€…ç®€å•å­—ç¬¦ä¸²
            import json
            try:
                # å°è¯•è§£æä¸º JSON
                if tool_input.strip().startswith('{'):
                    kwargs = json.loads(tool_input)
                else:
                    # å¯¹äºç®€å•å­—ç¬¦ä¸²è¾“å…¥ï¼Œå°è¯•æ¨æ–­å‚æ•°å
                    # å¤§å¤šæ•°æœç´¢ç±»å·¥å…·ä½¿ç”¨ 'query' ä½œä¸ºä¸»è¦å‚æ•°
                    kwargs = {"query": tool_input}
            except json.JSONDecodeError:
                # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
                kwargs = {"query": tool_input}
            
            logger.info(f"ğŸ”§ æ‰§è¡Œ MCP å·¥å…·: {tool_name}, å‚æ•°: {kwargs}")
            
            # æ‰§è¡Œå·¥å…·ï¼ˆå¸¦è¶…æ—¶ï¼‰
            result = await asyncio.wait_for(
                tool._arun(**kwargs),
                timeout=self.config.tool_timeout
            )
            
            if not result or result.strip() == "":
                return f"MCP tool '{tool_name}' returned no results."
            
            return result
            
        except asyncio.TimeoutError:
            logger.error(f"MCP tool '{tool_name}' timed out")
            return f"[ERROR] MCP tool '{tool_name}' timed out after {self.config.tool_timeout}s"
        except Exception as e:
            logger.error(f"MCP tool '{tool_name}' error: {str(e)}", exc_info=True)
            return f"[ERROR] MCP tool '{tool_name}' failed: {str(e)}"
    
    async def _execute_recall(self, query: str, state: AgentState) -> str:
        """æ‰§è¡Œæ–‡æ¡£å¬å›"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„ recall_tool
            result = await asyncio.wait_for(
                asyncio.to_thread(self.recall_tool._run, query),
                timeout=self.config.tool_timeout
            )
            
            if not result or result.strip() == "":
                return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"
            
            return result
        except Exception as e:
            logger.error(f"Recall error: {str(e)}")
            return f"[ERROR] æ–‡æ¡£å¬å›å¤±è´¥: {str(e)}"
    
    async def _execute_web_search(self, query: str) -> str:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        if not self.web_search_tool:
            return "[ERROR] ç½‘ç»œæœç´¢åŠŸèƒ½æœªå¯ç”¨ã€‚"
        
        try:
            result = await asyncio.wait_for(
                self.web_search_tool._arun(query),
                timeout=self.config.tool_timeout
            )
            
            if not result or result.strip() == "":
                return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚"
            
            return result
        except Exception as e:
            logger.error(f"Web search error: {str(e)}")
            return f"[ERROR] ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}"
    
    def _generate_forced_answer(self, scratchpad: Scratchpad, user_query: str) -> str:
        """å½“è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°æ—¶ï¼ŒåŸºäºå·²æ”¶é›†ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ"""
        if len(scratchpad) == 0:
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚è¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„é—®é¢˜ã€‚"
        
        # æ”¶é›†æ‰€æœ‰ observation
        observations = []
        for entry in scratchpad.entries:
            if entry.observation and not entry.observation.startswith("[ERROR]"):
                observations.append(entry.observation)
        
        if not observations:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨å°è¯•å›ç­”æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ã€‚è¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„é—®é¢˜ã€‚"
        
        # ç®€å•æ‹¼æ¥å·²æ”¶é›†çš„ä¿¡æ¯
        collected_info = "\n\n".join(observations[:3])  # æœ€å¤šå–å‰3ä¸ª
        return f"åŸºäºæˆ‘æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼š\n\n{collected_info}\n\nï¼ˆæ³¨ï¼šç”±äºæ¨ç†æ­¥æ•°é™åˆ¶ï¼Œç­”æ¡ˆå¯èƒ½ä¸å®Œæ•´ï¼‰"
    
    def _chunk_text(self, text: str, chunk_size: int = 50) -> list:
        """å°†æ–‡æœ¬åˆ†å—ç”¨äºæµå¼è¾“å‡º"""
        chunks = []
        for i in range(0, len(text), chunk_size):
            chunks.append(text[i:i + chunk_size])
        return chunks
