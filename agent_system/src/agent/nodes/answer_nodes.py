"""ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹"""
import asyncio
from typing import Dict, Any, AsyncGenerator

from langchain_core.messages import HumanMessage, AIMessage

from .base import BaseAgentNode
from ..state import AgentState, IntentType
from ..constants import MAX_CONCURRENT_LLM_CALLS
from ...prompts import (
    SINGLE_DOC_SUMMARY_PROMPT,
    MULTI_DOC_SUMMARY_PROMPT,
    REVIEW_GENERATION_PROMPT,
    LITERATURE_QA_PROMPT,
    DOCUMENT_COMPARISON_PROMPT,
    MULTI_DOC_SUMMARY_SYNTHESIS_PROMPT,
    REVIEW_GENERATION_SYNTHESIS_PROMPT,
    MULTI_DOC_SUMMARY_FINAL_MERGE_PROMPT,
    REVIEW_GENERATION_FINAL_MERGE_PROMPT,
)
from ...utils.logger import get_logger

logger = get_logger(__name__)


class AnswerNodes(BaseAgentNode):
    """ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹"""
    
    async def answer_generation_node_stream(
        self,
        state: AgentState
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹"""
        try:
            intent = state['detected_intent']
            doc_count = state.get('doc_count', 0)
            strategy = state.get('strategy')
            
            # å¤šæ–‡æ¡£æ€»ç»“æ¨¡å¼
            if strategy == "multi_doc_summary":
                async for event in self._handle_multi_doc_summary(state, intent):
                    yield event
                return
            
            # å…¶ä»–æ¨¡å¼
            context_str = await self._get_conversation_context_async(state, stage="answer_generation")
            context_for_llm = self._build_collected_info_for_answer(state)
            
            # é€‰æ‹©æç¤ºè¯
            prompt_template = self._select_prompt_template(intent, doc_count)
            
            prompt = prompt_template.format(
                user_query=state["user_query"],
                conversation_history=context_str if context_str else "æ— ",
                documents_content=context_for_llm if context_for_llm else "æ— "
            )
            
            full_answer = ""
            async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_answer += chunk_content
                yield {"type": "answer_chunk", "node": "answer_generation", "content": chunk_content}
            
            # ä¿å­˜ä¼šè¯ï¼ˆä½¿ç”¨çº¿ç¨‹æ± é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            session_id = state.get("session_id")
            if session_id:
                model_name = self.llm.model_name if hasattr(self.llm, 'model_name') else "unknown"
                
                if not state.get("_user_message_saved"):
                    await asyncio.to_thread(
                        self.session_manager.add_user_message,
                        session_id=session_id,
                        content=state["user_query"],
                        model_name=model_name
                    )
                
                await asyncio.to_thread(
                    self.session_manager.add_assistant_message,
                    session_id=session_id,
                    content=full_answer,
                    model_name=model_name
                )
            
            result = {
                "final_answer": full_answer,
                "messages": state.get("messages", []) + [AIMessage(content=full_answer)]
            }
            
            yield {"type": "node_complete", "data": result}
        except Exception as e:
            logger.error(f"Error in answer_generation_node_stream: {str(e)}", exc_info=True)
            yield {"type": "node_error", "node": "answer_generation", "error": str(e)}
    
    def _select_prompt_template(self, intent: IntentType, doc_count: int) -> str:
        """
        é€‰æ‹©æç¤ºè¯æ¨¡æ¿
        
        Args:
            intent: æ„å›¾ç±»å‹
            doc_count: æ–‡æ¡£æ•°é‡
            
        Returns:
            æç¤ºè¯æ¨¡æ¿å­—ç¬¦ä¸²
        """
        if intent == IntentType.LITERATURE_SUMMARY:
            return SINGLE_DOC_SUMMARY_PROMPT if doc_count == 1 else MULTI_DOC_SUMMARY_PROMPT
        elif intent == IntentType.REVIEW_GENERATION:
            return REVIEW_GENERATION_PROMPT
        elif intent == IntentType.LITERATURE_QA:
            return LITERATURE_QA_PROMPT
        elif intent == IntentType.DOCUMENT_COMPARISON:
            return DOCUMENT_COMPARISON_PROMPT
        else:
            # GENERAL_TASK ä¸åº”è¯¥èµ°åˆ°è¿™é‡Œï¼Œç”± ReAct å¤„ç†
            return LITERATURE_QA_PROMPT
    
    async def _handle_multi_doc_summary(
        self,
        state: AgentState,
        intent: IntentType
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†å¤šæ–‡æ¡£æ€»ç»“
        
        å¤„ç†æµç¨‹ï¼š
        1. å•ç»„åœºæ™¯ï¼šç›´æ¥ä½¿ç”¨ synthesis æç¤ºè¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        2. å¤šç»„åœºæ™¯ï¼š
           - å…ˆä¸ºæ¯ç»„ç”Ÿæˆä¸­é—´æŠ¥å‘Šï¼ˆä¸æµå¼è¾“å‡ºï¼‰
           - å†ä½¿ç”¨ final_merge æç¤ºè¯åˆå¹¶æ‰€æœ‰æŠ¥å‘Šï¼ˆæµå¼è¾“å‡ºï¼‰
        
        Args:
            state: Agent çŠ¶æ€
            intent: æ„å›¾ç±»å‹
            
        Yields:
            æµå¼äº‹ä»¶å­—å…¸
        """
        document_summaries = state.get("document_summaries", {})
        if not document_summaries:
            yield {"type": "error", "error": "No document summaries found"}
            return
        
        document_names = state.get("document_names", {}) or {}
        
        # æ–‡ç« å¯¹æ¯”ï¼šä¸åˆ†ç»„ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£æ€»ç»“
        if intent == IntentType.DOCUMENT_COMPARISON:
            summaries_text = "\n\n".join([
                f"## æ–‡æ¡£ {i+1}: {document_names.get(doc_id, doc_id)}\n{summary}"
                for i, (doc_id, summary) in enumerate(document_summaries.items())
            ])
            
            prompt = DOCUMENT_COMPARISON_PROMPT.format(
                user_query=state["user_query"],
                documents_summaries=summaries_text
            )
            
            full_answer = ""
            async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_answer += chunk_content
                yield {"type": "answer_chunk", "content": chunk_content}
            
            # ä¿å­˜ä¼šè¯
            await self._save_session_messages(state, full_answer)
            
            yield {
                "type": "final_answer",
                "data": {
                    "answer": full_answer,
                    "session_id": state.get("session_id", ""),
                    "detected_intent": intent.value if intent else "",
                    "follow_up_questions": state.get("follow_up_questions", [])
                }
            }
            return
        
        max_context_tokens = state.get("max_context_tokens", 100000)
        threshold = int(max_context_tokens * 0.7)
        
        groups = self._smart_split_document_summaries(document_summaries, threshold)
        
        # é€‰æ‹©æç¤ºè¯æ¨¡æ¿
        if intent == IntentType.LITERATURE_SUMMARY:
            synthesis_prompt_template = MULTI_DOC_SUMMARY_SYNTHESIS_PROMPT
            final_merge_prompt_template = MULTI_DOC_SUMMARY_FINAL_MERGE_PROMPT
        elif intent == IntentType.REVIEW_GENERATION:
            synthesis_prompt_template = REVIEW_GENERATION_SYNTHESIS_PROMPT
            final_merge_prompt_template = REVIEW_GENERATION_FINAL_MERGE_PROMPT
        else:
            yield {"type": "error", "error": f"Unexpected intent: {intent}"}
            return
        
        document_names = state.get("document_names", {}) or {}
        
        if len(groups) == 1:
            # å•ç»„åœºæ™¯ï¼šç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            summaries_text = "\n\n".join([
                f"## æ–‡æ¡£ {i+1}: {document_names.get(doc_id, doc_id)}\n{summary}"
                for i, (doc_id, summary) in enumerate(groups[0])
            ])
            
            prompt = synthesis_prompt_template.format(
                user_query=state["user_query"],
                documents_summaries=summaries_text
            )
            
            full_answer = ""
            async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_answer += chunk_content
                yield {"type": "answer_chunk", "content": chunk_content}
            
            # ä¿å­˜ä¼šè¯
            await self._save_session_messages(state, full_answer)
            
            yield {
                "type": "final_answer",
                "data": {
                    "answer": full_answer,
                    "session_id": state.get("session_id", ""),
                    "detected_intent": intent.value if intent else "",
                    "follow_up_questions": state.get("follow_up_questions", [])
                }
            }
        else:
            # å¤šç»„åœºæ™¯ï¼šå…ˆç”Ÿæˆä¸­é—´æŠ¥å‘Šï¼Œå†åˆå¹¶
            logger.info(f"ğŸ“š å¤šç»„å¤„ç†æ¨¡å¼ï¼š{len(groups)} ç»„ï¼Œå¼€å§‹ç”Ÿæˆä¸­é—´æŠ¥å‘Š...")
            logger.info(f"âš™ï¸ å¹¶å‘é™åˆ¶: æœ€å¤šåŒæ—¶å¤„ç† {MAX_CONCURRENT_LLM_CALLS} ç»„")
            
            # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM_CALLS)
            
            async def generate_group_report(idx: int, group: list) -> tuple:
                """å¸¦ä¿¡å·é‡é™åˆ¶çš„åˆ†ç»„æŠ¥å‘Šç”Ÿæˆ"""
                async with semaphore:
                    summaries_text = "\n\n".join([
                        f"## æ–‡æ¡£: {document_names.get(doc_id, doc_id)}\n{summary}"
                        for doc_id, summary in group
                    ])
                    prompt = synthesis_prompt_template.format(
                        user_query=state["user_query"],
                        documents_summaries=summaries_text
                    )
                    
                    # ä¸æµå¼è¾“å‡ºä¸­é—´æŠ¥å‘Šï¼Œåªæ”¶é›†ç»“æœ
                    group_answer = ""
                    async for chunk in self.llm.astream([HumanMessage(content=prompt)]):
                        chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                        group_answer += chunk_content
                    
                    logger.info(f"   âœ… ç¬¬ {idx + 1}/{len(groups)} ç»„æŠ¥å‘Šå®Œæˆï¼Œé•¿åº¦: {len(group_answer)}")
                    return idx, group_answer
            
            # å¹¶è¡Œç”Ÿæˆæ‰€æœ‰åˆ†ç»„æŠ¥å‘Šï¼ˆå—ä¿¡å·é‡é™åˆ¶ï¼‰
            tasks = [generate_group_report(idx, group) for idx, group in enumerate(groups)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æŒ‰ç´¢å¼•æ’åºç»“æœ
            group_reports = [""] * len(groups)
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"åˆ†ç»„æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {result}")
                else:
                    idx, report = result
                    group_reports[idx] = report
            
            logger.info(f"ğŸ“ å¼€å§‹åˆå¹¶ {len(group_reports)} ä¸ªåˆ†ç»„æŠ¥å‘Š...")
            
            # ä½¿ç”¨ä¸“é—¨çš„åˆå¹¶æç¤ºè¯
            all_reports_text = "\n\n".join([
                f"# ç¬¬ {i+1} ç»„åˆ†ææŠ¥å‘Š\n{report}"
                for i, report in enumerate(group_reports)
            ])
            final_prompt = final_merge_prompt_template.format(
                user_query=state["user_query"],
                group_reports=all_reports_text
            )
            
            # åªæµå¼è¾“å‡ºæœ€ç»ˆåˆå¹¶ç»“æœ
            full_answer = ""
            async for chunk in self.llm.astream([HumanMessage(content=final_prompt)]):
                chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_answer += chunk_content
                yield {"type": "answer_chunk", "content": chunk_content}
            
            # ä¿å­˜ä¼šè¯
            await self._save_session_messages(state, full_answer)
            
            yield {
                "type": "final_answer",
                "data": {
                    "answer": full_answer,
                    "session_id": state.get("session_id", ""),
                    "detected_intent": intent.value if intent else "",
                    "follow_up_questions": state.get("follow_up_questions", [])
                }
            }
    
    async def _save_session_messages(self, state: AgentState, answer: str) -> None:
        """
        ä¿å­˜ä¼šè¯æ¶ˆæ¯ï¼ˆç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹æ¶ˆæ¯ï¼‰
        
        Args:
            state: Agent çŠ¶æ€
            answer: åŠ©æ‰‹å›ç­”å†…å®¹
        """
        session_id = state.get("session_id")
        if not session_id:
            return
        
        model_name = self.llm.model_name if hasattr(self.llm, 'model_name') else "unknown"
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¦‚æœå°šæœªä¿å­˜ï¼‰
        if not state.get("_user_message_saved"):
            await asyncio.to_thread(
                self.session_manager.add_user_message,
                session_id=session_id,
                content=state["user_query"],
                model_name=model_name
            )
        
        # ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
        await asyncio.to_thread(
            self.session_manager.add_assistant_message,
            session_id=session_id,
            content=answer,
            model_name=model_name
        )
